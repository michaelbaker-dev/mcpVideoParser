{
  "storage": {
    "base_path": "./video_data",
    "max_cache_size_gb": 50,
    "cleanup_after_days": 30
  },
  "processing": {
    "frame_sample_rate": 30,
    "enable_scene_detection": true,
    "scene_threshold": 0.3,
    "max_frames_per_video": 1000,
    "frame_quality": 85,
    "frame_format": "jpg",
    "audio": {
      "model": "base",
      "language": "en",
      "enable_timestamps": true
    }
  },
  "llm": {
    "ollama_host": "http://localhost:11434",
    "vision_model": "llava:latest",
    "text_model": "llama2:latest",
    "embedding_model": "nomic-embed-text",
    "max_context_length": 4096,
    "temperature": 0.7,
    "timeout_seconds": 60,
    "_comment_vision_model": "The vision_model (like llava:latest) is used for analyzing video frames and understanding visual content",
    "_comment_text_model": "The text_model (like llama2:latest) is used for generating summaries, answering questions, and chat interactions",
    "_comment_chat_vs_video": "Chat LLM is specified when starting the client with --chat-llm flag. Video LLM is this text_model config."
  },
  "performance": {
    "max_concurrent_videos": 2,
    "frame_batch_size": 10,
    "enable_gpu": true,
    "thread_count": 4
  },
  "logging": {
    "level": "INFO",
    "format": "json",
    "file": "logs/mcp-video-server.log"
  }
}